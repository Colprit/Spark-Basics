{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "round-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext, HiveContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smart-retail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://20c6acaf8821:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sandbox</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc5441ecf70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.local.dir: default /tmp (overridden by SPARK_LOCAL_DIRS)\n",
    "# spark.sql.warehouse.dir: default $PWD/spark-warehouse\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"sandbox\") \\\n",
    "        .config(\"spark.local.dir\", \"/home/jovyan/spark_local_dir/\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/home/jovyan/spark-warehouse\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dderby.stream.error.file=/home/jovyan\") \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dderby.system.home=/home/jovyan/derby_sys\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "stretch-third",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0 - 10 mph: string (nullable = true)\n",
      " |-- 0 - 520 cm: string (nullable = true)\n",
      " |-- 11 - 15 mph: string (nullable = true)\n",
      " |-- 1160+ cm: string (nullable = true)\n",
      " |-- 16 - 20 mph: string (nullable = true)\n",
      " |-- 21 - 25 mph: string (nullable = true)\n",
      " |-- 26 - 30 mph: string (nullable = true)\n",
      " |-- 31 - 35 mph: string (nullable = true)\n",
      " |-- 36 - 40 mph: string (nullable = true)\n",
      " |-- 41 - 45 mph: string (nullable = true)\n",
      " |-- 46 - 50 mph: string (nullable = true)\n",
      " |-- 51 - 55 mph: string (nullable = true)\n",
      " |-- 521 - 660 cm: string (nullable = true)\n",
      " |-- 56 - 60 mph: string (nullable = true)\n",
      " |-- 61 - 70 mph: string (nullable = true)\n",
      " |-- 661 - 1160 cm: string (nullable = true)\n",
      " |-- 71 - 80 mph: string (nullable = true)\n",
      " |-- 80+ mph: string (nullable = true)\n",
      " |-- Avg mph: string (nullable = true)\n",
      " |-- Report Date: string (nullable = true)\n",
      " |-- Site Name: string (nullable = true)\n",
      " |-- Time Interval: string (nullable = true)\n",
      " |-- Time Period Ending: string (nullable = true)\n",
      " |-- Total Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_filename = 'report_daily_8188_31012020_31072020.json'\n",
    "\n",
    "raw_df = spark.read.json(report_filename)\n",
    "\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "loaded-colorado",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------\n",
      " 0 - 10 mph         |                     \n",
      " 0 - 520 cm         | 12                  \n",
      " 11 - 15 mph        |                     \n",
      " 1160+ cm           | 6                   \n",
      " 16 - 20 mph        |                     \n",
      " 21 - 25 mph        |                     \n",
      " 26 - 30 mph        |                     \n",
      " 31 - 35 mph        |                     \n",
      " 36 - 40 mph        |                     \n",
      " 41 - 45 mph        |                     \n",
      " 46 - 50 mph        |                     \n",
      " 51 - 55 mph        |                     \n",
      " 521 - 660 cm       | 0                   \n",
      " 56 - 60 mph        |                     \n",
      " 61 - 70 mph        |                     \n",
      " 661 - 1160 cm      | 1                   \n",
      " 71 - 80 mph        |                     \n",
      " 80+ mph            |                     \n",
      " Avg mph            | 63                  \n",
      " Report Date        | 2020-01-31T00:00:00 \n",
      " Site Name          | 7001/1              \n",
      " Time Interval      | 0                   \n",
      " Time Period Ending | 00:14:00            \n",
      " Total Volume       | 19                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "compressed-playlist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+------------------+----------+------------+-------------+--------+------------+-------+\n",
      "|        Report Date|Time Interval|Time Period Ending|0 - 520 cm|521 - 660 cm|661 - 1160 cm|1160+ cm|Total Volume|Avg mph|\n",
      "+-------------------+-------------+------------------+----------+------------+-------------+--------+------------+-------+\n",
      "|2020-01-31T00:00:00|            0|          00:14:00|        12|           0|            1|       6|          19|     63|\n",
      "|2020-01-31T00:00:00|            1|          00:29:00|         8|           1|            0|       5|          14|     54|\n",
      "|2020-01-31T00:00:00|            2|          00:44:00|         0|           0|            0|       6|           6|     58|\n",
      "|2020-01-31T00:00:00|            3|          00:59:00|         6|           0|            0|       3|           9|     59|\n",
      "|2020-01-31T00:00:00|            4|          01:14:00|         7|           0|            0|       8|          15|     60|\n",
      "|2020-01-31T00:00:00|            5|          01:29:00|         7|           2|            0|       4|          13|     59|\n",
      "|2020-01-31T00:00:00|            6|          01:44:00|         3|           0|            0|       5|           8|     56|\n",
      "|2020-01-31T00:00:00|            7|          01:59:00|         3|           2|            0|       6|          11|     60|\n",
      "|2020-01-31T00:00:00|            8|          02:14:00|         1|           0|            1|       3|           5|     61|\n",
      "|2020-01-31T00:00:00|            9|          02:29:00|         2|           0|            1|       6|           9|     60|\n",
      "|2020-01-31T00:00:00|           10|          02:44:00|         4|           0|            0|      10|          14|     56|\n",
      "|2020-01-31T00:00:00|           11|          02:59:00|         3|           0|            1|       5|           9|     61|\n",
      "|2020-01-31T00:00:00|           12|          03:14:00|         0|           2|            2|       3|           7|     51|\n",
      "|2020-01-31T00:00:00|           13|          03:29:00|         1|           2|            2|       3|           8|     53|\n",
      "|2020-01-31T00:00:00|           14|          03:44:00|         4|           1|            2|       7|          14|     58|\n",
      "|2020-01-31T00:00:00|           15|          03:59:00|         3|           1|            1|       4|           9|     58|\n",
      "|2020-01-31T00:00:00|           16|          04:14:00|         8|           0|            1|       4|          13|     61|\n",
      "|2020-01-31T00:00:00|           17|          04:29:00|         3|           2|            1|       8|          14|     58|\n",
      "|2020-01-31T00:00:00|           18|          04:44:00|         9|           2|            1|       5|          17|     64|\n",
      "|2020-01-31T00:00:00|           19|          04:59:00|        12|           1|            0|      10|          23|     60|\n",
      "+-------------------+-------------+------------------+----------+------------+-------------+--------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "#     'Site Name',  # 7001/1\n",
    "    'Report Date',\n",
    "    'Time Interval',\n",
    "    'Time Period Ending',\n",
    "    '0 - 520 cm',\n",
    "    '521 - 660 cm',\n",
    "    '661 - 1160 cm',\n",
    "    '1160+ cm',\n",
    "    'Total Volume',\n",
    "    'Avg mph'\n",
    "]\n",
    "\n",
    "report_df = raw_df.select(columns)\n",
    "\n",
    "report_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "homeless-still",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Report Date: string (nullable = true)\n",
      " |-- Time Interval: integer (nullable = true)\n",
      " |-- Time Period Ending: string (nullable = true)\n",
      " |-- 0 - 520 cm: integer (nullable = true)\n",
      " |-- 521 - 660 cm: integer (nullable = true)\n",
      " |-- 661 - 1160 cm: integer (nullable = true)\n",
      " |-- 1160+ cm: integer (nullable = true)\n",
      " |-- Total Volume: integer (nullable = true)\n",
      " |-- Avg mph: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cast types\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cast_as_ints = [\n",
    "    'Time Interval',\n",
    "    '0 - 520 cm',\n",
    "    '521 - 660 cm',\n",
    "    '661 - 1160 cm',\n",
    "    '1160+ cm',\n",
    "    'Total Volume',\n",
    "    'Avg mph'\n",
    "]\n",
    "\n",
    "for column in cast_as_ints:\n",
    "    \n",
    "    report_df = report_df.withColumn(column, col(column).cast('int'))\n",
    "\n",
    "\n",
    "report_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-egypt",
   "metadata": {},
   "source": [
    "### Reading using a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "pursuant-fishing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Site Name: string (nullable = true)\n",
      " |-- Report Date: timestamp (nullable = true)\n",
      " |-- Time Period Ending: timestamp (nullable = true)\n",
      " |-- Time Interval: string (nullable = true)\n",
      " |-- 0 - 520 cm: string (nullable = true)\n",
      " |-- 521 - 660 cm: string (nullable = true)\n",
      " |-- 661 - 1160 cm: string (nullable = true)\n",
      " |-- 1160+ cm: string (nullable = true)\n",
      " |-- Total Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    DateType\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('Site Name', StringType(), False),\n",
    "    StructField('Report Date', TimestampType(), True),\n",
    "    StructField('Time Period Ending', TimestampType(), True),\n",
    "    StructField('Time Interval', StringType(), True),\n",
    "    StructField('0 - 520 cm', StringType(), True),\n",
    "    StructField('521 - 660 cm', StringType(), True),\n",
    "    StructField('661 - 1160 cm', StringType(), True),\n",
    "    StructField('1160+ cm', StringType(), True),\n",
    "    StructField('Total Volume', StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "from_schema_df = spark \\\n",
    "            .read \\\n",
    "            .schema(schema) \\\n",
    "            .json(report_filename)\n",
    "\n",
    "from_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "boring-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+-------------+----------+------------+-------------+--------+------------+\n",
      "|Site Name|        Report Date| Time Period Ending|Time Interval|0 - 520 cm|521 - 660 cm|661 - 1160 cm|1160+ cm|Total Volume|\n",
      "+---------+-------------------+-------------------+-------------+----------+------------+-------------+--------+------------+\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 00:14:00|            0|        12|           0|            1|       6|          19|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 00:29:00|            1|         8|           1|            0|       5|          14|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 00:44:00|            2|         0|           0|            0|       6|           6|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 00:59:00|            3|         6|           0|            0|       3|           9|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 01:14:00|            4|         7|           0|            0|       8|          15|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 01:29:00|            5|         7|           2|            0|       4|          13|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 01:44:00|            6|         3|           0|            0|       5|           8|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 01:59:00|            7|         3|           2|            0|       6|          11|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 02:14:00|            8|         1|           0|            1|       3|           5|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 02:29:00|            9|         2|           0|            1|       6|           9|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 02:44:00|           10|         4|           0|            0|      10|          14|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 02:59:00|           11|         3|           0|            1|       5|           9|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 03:14:00|           12|         0|           2|            2|       3|           7|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 03:29:00|           13|         1|           2|            2|       3|           8|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 03:44:00|           14|         4|           1|            2|       7|          14|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 03:59:00|           15|         3|           1|            1|       4|           9|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 04:14:00|           16|         8|           0|            1|       4|          13|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 04:29:00|           17|         3|           2|            1|       8|          14|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 04:44:00|           18|         9|           2|            1|       5|          17|\n",
      "|   7001/1|2020-01-31 00:00:00|2021-05-06 04:59:00|           19|        12|           1|            0|      10|          23|\n",
      "+---------+-------------------+-------------------+-------------+----------+------------+-------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from_schema_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-romantic",
   "metadata": {},
   "source": [
    "### Simple Aggergations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "southeast-gregory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|Hour|sum(0 - 520 cm)|\n",
      "+----+---------------+\n",
      "|   1|           1967|\n",
      "|   2|           1438|\n",
      "|   3|           1778|\n",
      "|   4|           4113|\n",
      "|   5|          15748|\n",
      "|   6|          47194|\n",
      "|   7|          74069|\n",
      "|   8|          71276|\n",
      "|   9|          57701|\n",
      "|  10|          62063|\n",
      "|  11|          65850|\n",
      "|  12|          70568|\n",
      "|  13|          71711|\n",
      "|  14|          78743|\n",
      "|  15|          81461|\n",
      "|  16|          92098|\n",
      "|  17|          82934|\n",
      "|  18|          56308|\n",
      "|  19|          38152|\n",
      "|  20|          27341|\n",
      "|  21|          18723|\n",
      "|  22|          14311|\n",
      "|  23|           7849|\n",
      "|  24|           3945|\n",
      "+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# small vehicles by hour of the day\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "\n",
    "\n",
    "report_df \\\n",
    "    .withColumn('Hour', date_format(col(\"Time Period Ending\"), \"k\").cast('int')) \\\n",
    "    .groupBy('Hour') \\\n",
    "    .sum('0 - 520 cm') \\\n",
    "    .orderBy('Hour') \\\n",
    "    .show(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "upset-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|sum(Total Volume)|\n",
      "+-----+-----------------+\n",
      "|    1|            11172|\n",
      "|    2|           296970|\n",
      "|    3|           247328|\n",
      "|    4|            96891|\n",
      "|    5|           152918|\n",
      "|    6|           218275|\n",
      "|    7|           279321|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total volume by month\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "\n",
    "report_df \\\n",
    "    .withColumn('Month', month(report_df['Report Date'])) \\\n",
    "    .groupBy('Month') \\\n",
    "    .sum('Total Volume') \\\n",
    "    .orderBy('Month') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "entitled-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Week Day|Total Volume|\n",
      "+--------+------------+\n",
      "|     Fri|      215643|\n",
      "|     Thu|      205929|\n",
      "|     Wed|      201701|\n",
      "|     Tue|      198268|\n",
      "|     Mon|      193474|\n",
      "|     Sat|      150390|\n",
      "|     Sun|      137470|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total volume by day of the week\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "report_df \\\n",
    "    .withColumn(\"Week Day\", date_format(col(\"Report Date\"), \"E\")) \\\n",
    "    .groupBy('Week Day') \\\n",
    "    .sum('Total Volume') \\\n",
    "    .withColumnRenamed('Sum(Total Volume)', 'Total Volume') \\\n",
    "    .orderBy('Total Volume', ascending=False) \\\n",
    "    .show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
